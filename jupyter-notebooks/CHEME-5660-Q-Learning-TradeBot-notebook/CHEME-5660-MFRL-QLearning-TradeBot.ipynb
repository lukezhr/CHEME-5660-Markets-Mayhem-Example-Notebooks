{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5d38b9-d044-424b-b0ce-9104ae714630",
   "metadata": {},
   "source": [
    "## CHEME 5660: Building a Trading Bot using Model-Free Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51147f0c-66d0-402c-8b55-b249aab8eb6a",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this example we'll use [Q-learning](https://en.wikipedia.org/wiki/Q-learning), a model-free reinforcement learning approach, to In this example, we'll use [Q-learning](https://en.wikipedia.org/wiki/Q-learning), a model-free reinforcement learning approach, to build a trading bot for stocks in the `CHEME-5660 portfolio`. In particular, we've downloaded `5 min` Open High Low Close (OHLC) data for the 150 tickers in the `CHEME 5660 portfolio` from [Polygon.io](https://polygon.io). From this data, we'll use the Q-learning approach from [Chapter 17 of Kochenderfer et al. (2022)](https://algorithmsbook.com) to estimate the $Q(s, a)$ table; once we have the $Q(s, a)$ table, we can estimate the policy $\\pi(s)$:\n",
    "\n",
    "$$\\pi(s) = \\text{arg}\\max_{a}Q(s,a)$$\n",
    "\n",
    "Background reading/viewing on Reinforcement Learning and Model-free Reinforcement Learning:\n",
    "* [Chapter 17: Mykel J. Kochenderfer, Tim A. Wheeler, Kyle H. Wray \"Algorithms for Decision Making\", MIT Press 2022](https://algorithmsbook.com)\n",
    "* [Stanford CS234: Reinforcement Learning (2019), Lecture 4](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u)\n",
    "* [Stanford CS221: Artificial Intelligence: Principles and Techniques (2019), Lecture 8](https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf71fd-e5f2-48a6-adbc-c718214f7aef",
   "metadata": {},
   "source": [
    "### Example setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19553510-5bc0-43ad-9329-92ad26fc8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/jupyter-notebooks/CHEME-5660-Q-Learning-TradeBot-notebook`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/jupyter-notebooks/CHEME-5660-Q-Learning-TradeBot-notebook/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/jupyter-notebooks/CHEME-5660-Q-Learning-TradeBot-notebook/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\"); Pkg.resolve(); Pkg.instantiate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee5681f-0b5f-42a5-9de5-dc9eb1009ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load req packages -\n",
    "using DataFrames\n",
    "using Dates\n",
    "using FileIO\n",
    "using JLD2\n",
    "using PrettyTables\n",
    "using Distributions\n",
    "using Statistics\n",
    "using DataFrames\n",
    "using Plots\n",
    "using Colors\n",
    "using MLJLinearModels\n",
    "\n",
    "# setup paths -\n",
    "const _ROOT = pwd();\n",
    "const _PATH_TO_DATA = joinpath(_ROOT, \"data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bedf8e3-a1c0-43cd-ba6a-71a1843fd507",
   "metadata": {},
   "source": [
    "#### Load the example code library\n",
    "The call to the `include` function loads the `CHEME-5660-Example-CodeLib.jl` library into the notebook; this library contains types and functions we use during the example. In particular, we encode the online $Q(s,a)$ update routine listed as `Algorithm 17.2` from [Kochenderfer et al (2022)](https://algorithmsbook.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3215d94e-fc3c-4574-a066-a8138975fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"CHEME-5660-Example-CodeLib.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c0f2c-3705-4fc8-9089-f21bfb830e5f",
   "metadata": {},
   "source": [
    "### Setup constants and other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f244076-997b-4657-89de-b89e5477a492",
   "metadata": {},
   "source": [
    "#### Load and partition the OHLC price data set\n",
    "This data is `5 min` data, meaning we have Open High Low Close (OHLC) data for the 150 tickers in the `CHEME 5660 portfolio` every five minutes for one week of trading days. We'll use this data to estimate the trade policy of our agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c94029-3f64-425b-ab0c-89876157db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what ticker do we want to explore?\n",
    "ticker_symbol = \"AMD\";\n",
    "\n",
    "# load the JLD2 portfolio data file -\n",
    "price_data_dictionary = load(joinpath(_PATH_TO_DATA, \"CHEME-5660-Portfolio-Q-learning-5min-11-20-22.jld2\"))[\"dd\"];\n",
    "\n",
    "# we have these ticker symbols in our data set -\n",
    "ticker_symbol_array = sort(keys(price_data_dictionary) |> collect);\n",
    "\n",
    "# Partition the data into a training and prediction set\n",
    "(price_training_dict, price_prediction_dict) = partition(price_data_dictionary; fraction=0.90);\n",
    "\n",
    "# this version uses all the data for training -\n",
    "# df_training = price_data_dictionary[ticker_symbol];\n",
    "\n",
    "# this version uses only some of the data for training \n",
    "df_training = price_training_dict[ticker_symbol];\n",
    "df_prediction = price_prediction_dict[ticker_symbol];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c929f0-839c-40d9-a791-db6227e48044",
   "metadata": {},
   "source": [
    "##### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e39e8a-b45b-4181-9001-144e010fd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many days of historical data are we using?\n",
    "d = 1;       # we nₐ buy shares of XYZ\n",
    "nₐ = 1.0;    # how many shares do we want to buy, sell each day\n",
    "δ = 0.50;    # z-score cutoff\n",
    "\n",
    "# setup actions states -\n",
    "actions = [1,2,3]  ; # buy, sell, hold\n",
    "states = [1,2,3,4] ; # states defined below -\n",
    "\n",
    "# parameters for the Q(s,a) estimation -\n",
    "ϵ = 0.45;\n",
    "number_of_trials = 500;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8e99a9-4814-497b-8b02-0d0dfb6e3b5f",
   "metadata": {},
   "source": [
    "#### Define discrete state classes\n",
    "Let the state of our decision process be the share price of the underlying asset `XYZ`. However, the share price is (arguably) a continuous state variable, and we need to have discrete states. Toward this challenge, let's train a [Multiclass classifier](https://en.wikipedia.org/wiki/Multiclass_classification) that takes price as an input and gives back an integer class $\\mathcal{C} = \\left\\{1,2,\\dots,c\\right\\}$; where we build the following classes:\n",
    "\n",
    "* The share price $S_{t}$ belongs to `class 1` if it is close to an expected price $\\mathbb{E}\\left(S\\right)$, but larger than the expected price $S_{t}>\\mathbb{E}\\left(S\\right)$. \n",
    "* The share price $S_{t}$ belongs to `class 2` if it is much larger than an expected price $\\mathbb{E}\\left(S\\right)$, i.e., $S_{t}\\gg\\mathbb{E}\\left(S\\right)$. \n",
    "* The share price $S_{t}$ belongs to `class 3` if it is close to an expected price $\\mathbb{E}\\left(S\\right)$, but smaller than the expected price $S_{t}<\\mathbb{E}\\left(S\\right)$.\n",
    "* The share price $S_{t}$ belongs to `class 4` if it is much smaller than an expected price $\\mathbb{E}\\left(S\\right)$, i.e., $S_{t}\\ll\\mathbb{E}\\left(S\\right)$.\n",
    "\n",
    "The notion of `close`, `smaller` or `larger` needs to be more concrete; let's base these distances on the [z-score cutoff](https://en.wikipedia.org/wiki/Standard_score) parameter $\\delta$ defined in the constants section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb9d281-bb5e-40cf-980c-72152149e000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-term price of AMD is Sₒ = 74.47168145933033 USD/share with σ̂ = 1.4059735498911663 USD/share\n"
     ]
    }
   ],
   "source": [
    "# fit a distribution to vwap data -\n",
    "normal_price_distribution = fit_mle(Normal, df_training[:,:volume_weighted_average_price]);\n",
    "\n",
    "# get parameters -\n",
    "θ = params(normal_price_distribution);\n",
    "\n",
    "# setup price -\n",
    "Sₒ = θ[1];\n",
    "σ̂ = θ[2];\n",
    "\n",
    "# print -\n",
    "println(\"Long-term price of $(ticker_symbol) is Sₒ = $(Sₒ) USD/share with σ̂ = $(σ̂) USD/share\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a524428f-be81-4b27-af26-e74cfe2456b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a labled training data set:\n",
    "  \n",
    "# initialize -\n",
    "number_of_training_examples = nrow(df_training);\n",
    "number_of_column_labels = 2\n",
    "labeled_training_data = Array{Float64,2}(undef, number_of_training_examples,1);\n",
    "label_array = Array{Int64,1}(undef, number_of_training_examples);\n",
    "\n",
    "for i ∈ 1:number_of_training_examples\n",
    "    \n",
    "    # get the vwap -\n",
    "    vwap_value = df_training[i,:volume_weighted_average_price]; #vwap = volume weighted average price\n",
    "    labeled_training_data[i,1] = vwap_value;\n",
    "    label_array[i] = state(vwap_value; μ = Sₒ, σ = σ̂, δ = δ)\n",
    "end\n",
    "\n",
    "# build a multiclass classifier -\n",
    "mc_classifier_model = MultinomialRegression();\n",
    "theta = MLJLinearModels.fit(mc_classifier_model, labeled_training_data, label_array);\n",
    "W = reshape(theta,2,length(states));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e2bbe4-286e-480e-8c9b-164cfd1b9355",
   "metadata": {},
   "source": [
    "##### In-sample validation of the state classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24eee95a-e890-44cd-b3c6-cdd22dce9098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample correct classification fraction: 1.0\n"
     ]
    }
   ],
   "source": [
    "# compute the percent correct classification in sample -\n",
    "tmp_classification_array = Array{Int64,1}()\n",
    "for i ∈ 1:number_of_training_examples\n",
    "    \n",
    "    predicted_state_class = state(labeled_training_data[i,1], W);\n",
    "    actual_state_class = label_array[i];\n",
    "    \n",
    "    if (predicted_state_class == actual_state_class)\n",
    "        push!(tmp_classification_array,1);\n",
    "    else\n",
    "        push!(tmp_classification_array,0);\n",
    "    end\n",
    "end\n",
    "\n",
    "# how many 1's -\n",
    "correct_classification_fraction = (1/number_of_training_examples)*sum(tmp_classification_array);\n",
    "\n",
    "# correct -\n",
    "println(\"In sample correct classification fraction: $(correct_classification_fraction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bef9c3-2740-4d5a-8446-30ec51873150",
   "metadata": {},
   "source": [
    "##### Out-of-sample validation of the state classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44cc7882-5250-4368-89ee-7e465993b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of sample correct classification fraction: 1.0\n"
     ]
    }
   ],
   "source": [
    "# compute the percent correct classification in sample -\n",
    "tmp_ous_classification_array = Array{Int64,1}()\n",
    "number_of_prediction_examples = nrow(df_prediction);\n",
    "for i ∈ 1:number_of_prediction_examples\n",
    "    \n",
    "    # grab the price -\n",
    "    vwap_value = df_prediction[i,:volume_weighted_average_price];\n",
    "    \n",
    "    # compute the predicted class using the classifier -\n",
    "    predicted_state_class = state(vwap_value, W);\n",
    "    \n",
    "    # compute the actual class using the manual method -\n",
    "    actual_state_class = state(vwap_value; μ = Sₒ, σ = σ̂, δ = δ);\n",
    "    \n",
    "    if (predicted_state_class == actual_state_class)\n",
    "        push!(tmp_ous_classification_array,1);\n",
    "    else\n",
    "        push!(tmp_ous_classification_array,0);\n",
    "    end\n",
    "end\n",
    "\n",
    "# how many 1's -\n",
    "correct_classification_ous_fraction = (1/number_of_prediction_examples)*sum(tmp_ous_classification_array);\n",
    "\n",
    "# output -\n",
    "println(\"Out of sample correct classification fraction: $(correct_classification_ous_fraction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb9415-a963-491a-a9b7-fd2160074491",
   "metadata": {},
   "source": [
    "#### Can we break the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af43e5e6-1784-425a-83e4-d84ba2c188cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vwap = 73.5891 USD/share is class: 4 and predicted to be class: 4\n"
     ]
    }
   ],
   "source": [
    "# pick an index at random -\n",
    "idx_quick_look = 41;\n",
    "\n",
    "# grab the price -\n",
    "vwap_value = df_prediction[idx_quick_look,:volume_weighted_average_price];\n",
    "# vwap_value = df_training[idx_quick_look,:volume_weighted_average_price]; # look at training ...\n",
    "\n",
    "# compute the predicted class using the classifier -\n",
    "predicted_state_class = state(vwap_value, W);\n",
    "    \n",
    "# compute the actual class using the manual method -\n",
    "actual_state_class = state(vwap_value; μ = Sₒ, σ = σ̂, δ = δ);\n",
    "\n",
    "# println -\n",
    "println(\"vwap = $(vwap_value) USD/share is class: $(actual_state_class) and predicted to be class: $(predicted_state_class)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e749db-621e-4c9f-a770-90b8cb29355f",
   "metadata": {},
   "source": [
    "### Online estimation of the Q-table using Q-learning \n",
    "Now that we have our state classes (and our state classifier) let's turn our attention to the online estimation of the state-action-value matrix, otherwise known as the $ Q$ function. Imagine a scenario in which we don't know the transition matrix $T_{a}(s,s^{\\prime})$ or the rewards array $R(s, a)$. Instead, we let our system learn by trying different actions in different states and seeing what reward we get. Thus, we allow our agent to learn by example. \n",
    "\n",
    "In this example, we allow the agent to execute random actions (exploration) or execute its best guess of the optimal move, and then we observe the reward received. \n",
    "* The action space in this example is $\\mathcal{A}=\\left\\{\\mathtt{buy}, \\mathtt{sell}, \\mathtt{hold}\\right\\}$; thus, we either buy $n_{a}$ shares of `XYZ`, sell $n_{a}$ shares of `XYZ` or do nothing. \n",
    "* The state space $\\mathcal{S}$ is described above: $s_{1}$: current share close to but above some long-term expected price, $s_{2}$: current share price well above some long-term expected price, $s_{3}$: current share close to but below some long-term expected price and $s_{4}$: current share price well below some long-term expected price.\n",
    "* The reward we have chosen is the liquidation value on a per share basis of our position, i.e., what value could we get if we sold everything at the _next available market price_. In our case, we are using `5 min` data.\n",
    "\n",
    "#### Computational details\n",
    "* We do a random action $\\epsilon$ of the time; otherwise, we let the agent execute its best estimate of the optimal action.\n",
    "* We run `number_of_trials`, collect the $Q$-function for each trial and estimate a policy $\\pi(s)$ from the $Q(s, a)$ table for each trial. \n",
    "* After finishing `number_of_trials` and analyzing the data, we build a table with the recommended action for each state.\n",
    "* The update function was taken from `Algorithm 17.2` from [Kochenderfer et al. (2022)](https://algorithmsbook.com)\n",
    "* The current implementation of the code is slow. You've been warned. \n",
    "\n",
    "#### Interesting question:\n",
    "* I can imagine several `expert` policies, e.g., `buy the dip, sell the bounce`, a policy of 2,2,1,1. Alternatively, you could get `don't play small ball`, meaning wait for large swings to do anything (this has a policy of 3,2,3,1). Do we recover either of these policies? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3250b524-e6a3-4fd7-afc1-0fc1e51dd3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup ϵ sim -\n",
    "policy_array = Array{Int64,2}(undef, 4, number_of_trials);\n",
    "\n",
    "# setup categorical distribution for drawing a random action -\n",
    "action_distribution = Categorical([0.36,0.32,0.32]);\n",
    "\n",
    "for t ∈ 1:number_of_trials\n",
    "    \n",
    "    # initialize an empty ledger -\n",
    "    ledger_df = DataFrame(\n",
    "        time = DateTime[],\n",
    "        n = Float64[],\n",
    "        price = Float64[],\n",
    "        s = Int64[],\n",
    "        action = Int64[]\n",
    "    );\n",
    "\n",
    "    # initialize an empty Q -\n",
    "    Q_array = Array{Float64,2}(undef, length(states), length(actions));\n",
    "    fill!(Q_array, 0.0);\n",
    "    \n",
    "    # initial policy -> bias toward expert policy\n",
    "    Q_array[1,2] = 10.0;\n",
    "    Q_array[2,2] = 10.0;\n",
    "    Q_array[3,1] = 10.0;\n",
    "    Q_array[4,1] = 10.0;\n",
    "    \n",
    "    # build Q model -\n",
    "    QMODEL = QLearningModel();\n",
    "    QMODEL.γ = 0.80;\n",
    "    QMODEL.α = 0.05;\n",
    "    QMODEL.𝒮 = states;\n",
    "    QMODEL.𝒜 = actions;\n",
    "    QMODEL.Q = Q_array;\n",
    "    \n",
    "    # we buy 100 shares at initial price in the data -\n",
    "    transaction = (\n",
    "        time = df_training[1,:timestamp],\n",
    "        n = 100.0,\n",
    "        action = 1,\n",
    "        price = price(df_training,1),\n",
    "        s = state(price(df_training,1),W) \n",
    "    );\n",
    "    push!(ledger_df, transaction)\n",
    "    \n",
    "    # main random simulation -\n",
    "    for i ∈ 2:(nrow(df_training) - 1)\n",
    "\n",
    "        # get data from the df_training -\n",
    "        p = price(df_training, i);\n",
    "        p′ = price(df_training, i+1);\n",
    "\n",
    "        # convert that to the current state -\n",
    "        s = state(p, W);\n",
    "        s′ = state(p′, W);\n",
    "\n",
    "        # roll a random number -\n",
    "        if (rand() < ϵ)\n",
    "\n",
    "            # roll a random action - \n",
    "            aᵢ = rand(action_distribution);\n",
    "            if (aᵢ == 1) # random action: buy\n",
    "\n",
    "                # compute a buy action -\n",
    "                transaction = (\n",
    "                    time = df_training[i,:timestamp],\n",
    "                    n = nₐ,\n",
    "                    action = 1,\n",
    "                    price = p, \n",
    "                    s = s\n",
    "                );\n",
    "                push!(ledger_df, transaction)\n",
    "\n",
    "            elseif (aᵢ == 2) # random action: sell\n",
    "\n",
    "                # compute a buy action -\n",
    "                transaction = (\n",
    "                    time = df_training[i,:timestamp],\n",
    "                    n = nₐ,\n",
    "                    action = 2,\n",
    "                    price = p,\n",
    "                    s = s\n",
    "                );\n",
    "                push!(ledger_df, transaction)\n",
    "\n",
    "            elseif (aᵢ == 3) # random action: hold\n",
    "\n",
    "                # compute a buy action -\n",
    "                transaction = (\n",
    "                    time = df_training[i,:timestamp],\n",
    "                    n = nₐ,\n",
    "                    action = 3,\n",
    "                    price = p,\n",
    "                    s = s\n",
    "                );\n",
    "                push!(ledger_df, transaction)\n",
    "            end\n",
    "        else\n",
    "\n",
    "            # ok, what action does my best guess say that I should take?\n",
    "            policy = π(QMODEL.Q);\n",
    "            aᵢ = policy[s];\n",
    "\n",
    "            # compute a buy action -\n",
    "            transaction = (\n",
    "                time = df_training[i,:timestamp],\n",
    "                n = nₐ,\n",
    "                action = aᵢ,\n",
    "                price = p,\n",
    "                s = s\n",
    "            );\n",
    "            push!(ledger_df, transaction)\n",
    "        end\n",
    "        \n",
    "        # compute the total number of shares -\n",
    "        total_number_of_shares = compute_position_size(ledger_df);\n",
    "            \n",
    "        # we've update the ledger - compute the return per share if we sold everything at the price for the next time step -\n",
    "        Rᵢ = -1*total_number_of_shares*liquidate(ledger_df, p′);\n",
    "\n",
    "        # update the QMODEL -\n",
    "        update!(QMODEL, s, aᵢ, Rᵢ, s′);\n",
    "    end\n",
    "    \n",
    "    pvec = π(Q_array)\n",
    "    policy_array[1,t] = pvec[1]\n",
    "    policy_array[2,t] = pvec[2]\n",
    "    policy_array[3,t] = pvec[3]\n",
    "    policy_array[4,t] = pvec[4]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8af638df-114c-4c4b-932e-573b759f197e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────┬──────┬──────┬───────────────────┬────────────────────┬────────────────────┐\n",
      "│\u001b[1m Data (N=500): AMD  \u001b[0m│\u001b[1m      \u001b[0m│\u001b[1m      \u001b[0m│\u001b[1m                   \u001b[0m│\u001b[1m                    \u001b[0m│\u001b[1m                    \u001b[0m│\n",
      "│\u001b[90m                  s \u001b[0m│\u001b[90m π(s) \u001b[0m│\u001b[90m π̂(s) \u001b[0m│\u001b[90m Fraction a₁ (buy) \u001b[0m│\u001b[90m Fraction a₂ (sell) \u001b[0m│\u001b[90m Fraction a₃ (hold) \u001b[0m│\n",
      "├────────────────────┼──────┼──────┼───────────────────┼────────────────────┼────────────────────┤\n",
      "│                  1 │    2 │    2 │             0.288 │              0.368 │              0.344 │\n",
      "│                  2 │    2 │    2 │               0.3 │              0.364 │              0.336 │\n",
      "│                  3 │    1 │    2 │              0.35 │               0.36 │               0.29 │\n",
      "│                  4 │    1 │    1 │             0.686 │              0.086 │              0.228 │\n",
      "└────────────────────┴──────┴──────┴───────────────────┴────────────────────┴────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# show the policy table -\n",
    "\n",
    "# initialize -\n",
    "policy_table_data_array = Array{Any,2}(undef, 4, 6);\n",
    "for s ∈ 1:length(states)\n",
    "    \n",
    "    # compute -\n",
    "    policy_table_data_array[s,1] = s;\n",
    "    \n",
    "    # compute the fraction of a₁, a₂ and a₃ -\n",
    "    idx_a₁ = findall(x->x==1, policy_array[s,:]);\n",
    "    idx_a₂ = findall(x->x==2, policy_array[s,:]);\n",
    "    idx_a₃ = findall(x->x==3, policy_array[s,:]);\n",
    "    \n",
    "    # compute the fraction -\n",
    "    policy_table_data_array[s,4] = (length(idx_a₁)/number_of_trials);\n",
    "    policy_table_data_array[s,5] = (length(idx_a₂)/number_of_trials);\n",
    "    policy_table_data_array[s,6] = (length(idx_a₃)/number_of_trials);\n",
    "end\n",
    "\n",
    "# put expert in -\n",
    "policy_table_data_array[1,2] = 2;\n",
    "policy_table_data_array[2,2] = 2;\n",
    "policy_table_data_array[3,2] = 1;\n",
    "policy_table_data_array[4,2] = 1;\n",
    "\n",
    "# put recomended in -\n",
    "policy_table_data_array[1,3] = argmax(softmax(Vector{Float64}(policy_table_data_array[1,4:end])));\n",
    "policy_table_data_array[2,3] = argmax(softmax(Vector{Float64}(policy_table_data_array[2,4:end])));\n",
    "policy_table_data_array[3,3] = argmax(softmax(Vector{Float64}(policy_table_data_array[3,4:end])));\n",
    "policy_table_data_array[4,3] = argmax(softmax(Vector{Float64}(policy_table_data_array[4,4:end])));\n",
    "\n",
    "# header -\n",
    "header_data_policy_table = (\n",
    "    [\"Data (N=$(number_of_trials)): $(ticker_symbol) \", \"\", \"\", \"\", \"\", \"\"],\n",
    "    [\"s\", \"π(s)\", \"π̂(s)\",\"Fraction a₁ (buy)\", \"Fraction a₂ (sell)\", \"Fraction a₃ (hold)\"]\n",
    ");\n",
    "\n",
    "# display -\n",
    "pretty_table(policy_table_data_array; header=header_data_policy_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40ad53-9867-4f19-9603-8d1ec6eb3b16",
   "metadata": {},
   "source": [
    "__Table__: Recommended policy and the fraction of trials resulting in aᵢ for state s. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b8bd4c-14e2-49cc-a267-a673e6ad108f",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "* [Chapter 17: Mykel J. Kochenderfer, Tim A. Wheeler, Kyle H. Wray \"Algorithms for Decision Making\", MIT Press 2022](https://algorithmsbook.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4be7c3-9595-47cf-8594-8b34b1f4d2a5",
   "metadata": {},
   "source": [
    "### Disclaimer and Risks\n",
    "__This content is offered solely for training and  informational purposes__. No offer or solicitation to buy or sell securities or derivative products, or any investment or trading advice or strategy,  is made, given, or endorsed by the teaching team. \n",
    "\n",
    "__Trading involves risk__. Carefully review your financial situation before investing in securities, futures contracts, options, or commodity interests. Past performance, whether actual or indicated by historical tests of strategies, is no guarantee of future performance or success. Trading is generally inappropriate for someone with limited resources, investment or trading experience, or a low-risk tolerance.  Only risk capital that is not required for living expenses.\n",
    "\n",
    "__You are fully responsible for any investment or trading decisions you make__. Such decisions should be based solely on your evaluation of your financial circumstances, investment or trading objectives, risk tolerance, and liquidity needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
