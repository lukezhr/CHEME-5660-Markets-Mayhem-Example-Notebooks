{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35eb2a34-0c2a-435d-b8b9-2bd750239fbc",
   "metadata": {},
   "source": [
    "## CHEME 5660 Lab 8: Solution of the Linear Tiger Problem as a Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d3d6c-affb-444c-b52f-39c09c6af15a",
   "metadata": {},
   "source": [
    "<img src=\"./figs/Fig-Linear-MDP-Schematic.png\" style=\"margin:auto; width:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee4efe-e341-450a-bfbc-0e74d835afee",
   "metadata": {},
   "source": [
    "__Fig 1__: Schematic of the Tiger problem modeled as an N-state, two-action (left,right) Markov decision process. A tiger hides behind the green door while freedom awaits behind the red door."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d178f-067e-45d5-b16b-f3b21f1b5457",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "A Markov decision process is the tuple $\\left(\\mathcal{S}, \\mathcal{A}, R_{a}\\left(s, s^{\\prime}\\right), T_{a}\\left(s,s^{\\prime}\\right), \\gamma\\right)$ where:\n",
    "\n",
    "* The state space $\\mathcal{S}$ is the set of all possible states $s$ that a system can exist in\n",
    "* The action space $\\mathcal{A}$ is the set of all possible actions $a$ that are available to the agent, where $\\mathcal{A}_{s} \\subseteq \\mathcal{A}$ is the subset of the action space $\\mathcal{A}$ that is accessible from state $s$.\n",
    "* An expected immediate reward $R_{a}\\left(s, s^{\\prime}\\right)$ is received after transitioning from state $s\\rightarrow{s}^{\\prime}$ due to action $a$. \n",
    "* The transition $T_{a}\\left(s,s^{\\prime}\\right) = P(s_{t+1} = s^{\\prime}~|~s_{t}=s,a_{t} = a)$ denotes the probability that action $a$ in state $s$ at time $t$ will result in state $s^{\\prime}$ at time $t+1$\n",
    "* The quantity $\\gamma$ is a _discount factor_; the discount factor is used to weigh the _future expected utility_.\n",
    "\n",
    "Finally, a policy function $\\pi$ is the (potentially probabilistic) mapping from states $s\\in\\mathcal{S}$ to actions $a\\in\\mathcal{A}$ used by the agent to solve the decision task. \n",
    "\n",
    "### Policy evaluation\n",
    "One immediate question that jumps out is what is a policy function $\\pi$, and how do we find the best possible policy for our decision problem? To do this, we need a way to estimate how good (or bad) a particular policy is; the approach we use is called _policy evaluation_. Let's denote the expected utility gained by executing some policy $\\pi(s)$ from state $s$ as $U^{\\pi}(s)$. Then, an _optimal policy_ function $\\pi^{\\star}$ is one that maximizes the expected utility:\n",
    "\n",
    "\n",
    "$$\\pi^{\\star}\\left(s\\right) = \\text{arg} \\max_{\\pi}~U^{\\pi}(s)$$\n",
    "\n",
    "\n",
    "for all $s\\in\\mathcal{S}$. We can iteratively compute the utility of a policy $\\pi$. If the agent makes a single move, the utility will be the reward the agent receives by implementing policy $\\pi$:\n",
    "\n",
    "\n",
    "$$U_{1}^{\\pi}(s) = R(s,\\pi(s))$$\n",
    "\n",
    "\n",
    "However, if we let the agent perform two, three, or $k$ possible iterations, we get a _lookahead_ equation which relates the value of \n",
    "the utility at iteration $k$ to $k+1$:\n",
    "\n",
    "\n",
    "$$U_{k+1}^{\\pi}(s) = R(s,\\pi(s)) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T(s^{\\prime} | s, \\pi(s))U_{k}^{\\pi}(s^{\\prime})$$\n",
    "\n",
    "\n",
    "As $k\\rightarrow\\infty$ the lookahead utility converges to a stationary value $U^{\\pi}(s)$.\n",
    "\n",
    "### Q-functions\n",
    "Policy evaluation gives us a method to compute the utility for a particular policy $U^{\\pi}(s)$.  However, suppose we were given the utility and wanted to estimate the policy $\\pi(s)$ from that utility.  Given a utility $U(s)$, we can estimate a policy $\\pi(s)$ using the $Q$-function (action-value function):\n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T(s^{\\prime} | s, a)U(s^{\\prime})$$\n",
    "\n",
    "The $Q$-function a $|\\mathcal{S}|\\times|\\mathcal{A}|$ array, where the utility is given by:\n",
    "\n",
    "\n",
    "$$U(s) = \\max_{a} Q(s,a)$$\n",
    "\n",
    "and the policy $\\pi(s)$ is:\n",
    "\n",
    "$$\\pi(s) = \\text{arg}\\max_{a}Q(s,a)$$\n",
    "\n",
    "### Problem\n",
    "An agent is trapped in a long hallway with two doors at either end (Fig. 1). Behind the red door is a tiger (and certain death), while behind the green door is freedom. If the agent opens the red door, the agent is eaten (and receives a large negative reward). However, if the agent opens the green door, it escapes and gets a positive reward. \n",
    "\n",
    "For this problem, the MDP has the tuple components:\n",
    "* $\\mathcal{S} = \\left\\{1,2,\\dots,N\\right\\}$ while the action set is $\\mathcal{A} = \\left\\{a_{1},a_{2}\\right\\}$; action $a_{1}$ moves the agent one state to the left, action $a_{2}$ moves the agent one state to the right.\n",
    "* The agent receives a postive reward for entering state N (escapes). However, the agent is penalized for entering state 1 (eaten by the tiger). Finally, the agent is not charged to move to adjacent locations.\n",
    "* Let the probability of correctly executing an action $a_{j}\\in\\mathcal{A}$ be $\\alpha$.\n",
    "\n",
    "Let's compute $U^{\\pi}(s)$ for different choices for the policy function $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f1a0a-87f4-47d2-9a57-c0c187ad542f",
   "metadata": {},
   "source": [
    "## Lab 8 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8870fd6-db14-4025-bd34-346573da1dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\"); Pkg.resolve(); Pkg.instantiate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830ceeee-1638-4c48-b8a2-103614e58219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load req'd packages -\n",
    "using PrettyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01db2e1-9c7a-4a8c-bb92-20e7f63c769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"CHEME-5660-Lab-8-CodeLib.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e31a09-e973-483b-9609-c858de424830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some global constants -\n",
    "Œ± = 0.95; # probability of moving the direction we are expect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b1df8-e519-4ae9-836b-46113d43c095",
   "metadata": {},
   "source": [
    "#### Configure states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a3e1c5-bcee-4afd-beb1-cf6ca64ec09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the states and actions -\n",
    "start_index = 1;\n",
    "stop_index = 10;\n",
    "left_reward = -100.0;\n",
    "right_reward = 1000.0;\n",
    "\n",
    "states = range(start_index,stop=stop_index, step=1) |> collect;\n",
    "actions = [1,2]; # a‚ÇÅ = move left, a‚ÇÇ = move right\n",
    "Œ≥ = 0.95;\n",
    "\n",
    "# setup flags -\n",
    "should_run_T_check = false;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6a638-cb44-4a1e-8f0f-2baf3d0d8192",
   "metadata": {},
   "source": [
    "#### Configure the rewards array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4936603-3e2c-464c-8d9a-784121db2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the rewards -\n",
    "R = Array{Float64,2}(undef,length(states), length(actions));\n",
    "\n",
    "# most of the rewards are zero -\n",
    "fill!(R,0.0) # fill R w/zeros\n",
    "\n",
    "# set the rewards for the ends -\n",
    "R[start_index + 1, 1] = left_reward; # if in state 2, and we take action 1, we get eaten by a tiger. Bad.\n",
    "R[stop_index - 1, 2] = right_reward; # if in state N - 1, and we take action 2, we live, get married, have kids (who grow up to be Surgeons). Good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e76f5-a5a8-4f80-879b-536fd472dcfe",
   "metadata": {},
   "source": [
    "#### Configure the transition array $T_{a}(s,s^{\\prime})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de3b2301-0796-4664-8d31-d2d0e67f02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the transitions\n",
    "T = Array{Float64,3}(undef, length(states), length(states), length(actions));\n",
    "fill!(T,0.0);\n",
    "\n",
    "# We need to put values into the transition array (these are probabilities, so eah row much sum to 1)\n",
    "T[start_index, 1, 1:length(actions)] .= 1.0; # if we are in state 1, we stay in state 1 ‚àÄa ‚àà ùíú\n",
    "T[stop_index, stop_index, 1:length(actions)] .= 1.0; # if we are in state 5, we stay in state 5 \n",
    "\n",
    "# left actions -\n",
    "for s ‚àà 2:(stop_index - 1)\n",
    "    T[s,s-1,1] = Œ±;\n",
    "    T[s,s+1,1] = (1-Œ±);\n",
    "end\n",
    "\n",
    "# right actions -\n",
    "for s ‚àà 2:(stop_index - 1)\n",
    "    T[s,s-1,2] = (1-Œ±);\n",
    "    T[s,s+1,2] = Œ±; \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "316dfdc8-686c-4c98-b04a-cfdd648a8c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show we run the T-check?\n",
    "if (should_run_T_check == true)\n",
    "    \n",
    "    # row summation check -\n",
    "    T_array_check_table = Array{Any,2}(undef, length(states), length(actions)+1)\n",
    "\n",
    "    for s ‚àà 1:length(states)\n",
    "        T_array_check_table[s,1] = s;\n",
    "    end\n",
    "\n",
    "    for a ‚àà 1:length(actions)\n",
    "\n",
    "        # sum the action table -\n",
    "        Z = sum(T[:,:,a], dims=2)\n",
    "\n",
    "        for s ‚àà 1:length(states)\n",
    "            T_array_check_table[s,a+1] = Z[s]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # header -\n",
    "    T_check_header = ([\"State s\", \"a‚ÇÅ (left)\", \"a‚ÇÇ (right)\"]);\n",
    "\n",
    "    # display -\n",
    "    pretty_table(T_array_check_table; header=T_check_header)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e26bc1-2a74-451a-85cc-a6d18c0fa7c1",
   "metadata": {},
   "source": [
    "#### Build the MDP problem object and estimate the utility $U^{\\pi}(s)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea12f6c-3bc1-4c9f-9ed8-2c025b156a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_problem = build(MDPProblem; ùíÆ = states, ùíú = actions, T = T, R = R, Œ≥ = Œ≥);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58f6c6d4-307c-411a-b55c-10c032963259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a always left or always right policy -\n",
    "always_move_right(s) = 2;\n",
    "always_move_left(s) = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2a978a-aa71-4ef9-a4b2-221804db6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = iterative_policy_evaluation(mdp_problem, always_move_left, 20*length(states));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a59ef9b-9254-4ba7-9515-bd4c2257de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ\u001b[1m State s \u001b[0m‚îÇ\u001b[1m     U(s) \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ       1 ‚îÇ      0.0 ‚îÇ\n",
      "‚îÇ       2 ‚îÇ -104.699 ‚îÇ\n",
      "‚îÇ       3 ‚îÇ -98.9314 ‚îÇ\n",
      "‚îÇ       4 ‚îÇ -93.4814 ‚îÇ\n",
      "‚îÇ       5 ‚îÇ -88.3315 ‚îÇ\n",
      "‚îÇ       6 ‚îÇ  -83.465 ‚îÇ\n",
      "‚îÇ       7 ‚îÇ -78.8592 ‚îÇ\n",
      "‚îÇ       8 ‚îÇ  -74.358 ‚îÇ\n",
      "‚îÇ       9 ‚îÇ -67.1081 ‚îÇ\n",
      "‚îÇ      10 ‚îÇ      0.0 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "# display utility vector -\n",
    "utility_table_data_array = Array{Any,2}(undef, length(states), 2);\n",
    "\n",
    "# main table loop -\n",
    "for s ‚àà 1:length(states)\n",
    "    utility_table_data_array[s,1] = s\n",
    "    utility_table_data_array[s,2] = U[s]\n",
    "end\n",
    "\n",
    "# table header -\n",
    "utility_table_header = ([\"State s\", \"U(s)\"])\n",
    "\n",
    "# display -\n",
    "pretty_table(utility_table_data_array; header=utility_table_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169766e-885e-424b-a7e4-ad7dc292c828",
   "metadata": {},
   "source": [
    "#### Estimate the Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c20ce52-ade6-4766-aa10-3f00aabff501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Q array -\n",
    "Q_array = Q(mdp_problem, U)\n",
    "\n",
    "# compute the policy -\n",
    "policy = œÄ(Q_array);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ac4ae7-2442-472c-92a2-f097c4d613f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ\u001b[1m State \u001b[0m‚îÇ\u001b[1m Direction \u001b[0m‚îÇ\u001b[1m œÄ(s) \u001b[0m‚îÇ\u001b[1m  U(a‚ÇÅ) l \u001b[0m‚îÇ\u001b[1m  U(a‚ÇÇ) r \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ     1 ‚îÇ      stop ‚îÇ    0 ‚îÇ      0.0 ‚îÇ      0.0 ‚îÇ\n",
      "‚îÇ     2 ‚îÇ     right ‚îÇ    2 ‚îÇ -104.699 ‚îÇ -89.2856 ‚îÇ\n",
      "‚îÇ     3 ‚îÇ     right ‚îÇ    2 ‚îÇ -98.9314 ‚îÇ -89.3401 ‚îÇ\n",
      "‚îÇ     4 ‚îÇ     right ‚îÇ    2 ‚îÇ -93.4814 ‚îÇ -84.4184 ‚îÇ\n",
      "‚îÇ     5 ‚îÇ     right ‚îÇ    2 ‚îÇ -88.3315 ‚îÇ -79.7675 ‚îÇ\n",
      "‚îÇ     6 ‚îÇ     right ‚îÇ    2 ‚îÇ  -83.465 ‚îÇ -75.3662 ‚îÇ\n",
      "‚îÇ     7 ‚îÇ     right ‚îÇ    2 ‚îÇ -78.8592 ‚îÇ -71.0727 ‚îÇ\n",
      "‚îÇ     8 ‚îÇ     right ‚îÇ    2 ‚îÇ  -74.358 ‚îÇ -64.3109 ‚îÇ\n",
      "‚îÇ     9 ‚îÇ     right ‚îÇ    2 ‚îÇ -67.1081 ‚îÇ  996.468 ‚îÇ\n",
      "‚îÇ    10 ‚îÇ      stop ‚îÇ    0 ‚îÇ      0.0 ‚îÇ      0.0 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "# make a Q-table -\n",
    "Q_table_data_array = Array{Any,2}(undef, length(states), length(actions)+3)\n",
    "\n",
    "for s ‚àà 1:length(states)\n",
    "    \n",
    "    Q_table_data_array[s,1] = s;\n",
    "    \n",
    "    direction = \"left\"\n",
    "    policy_index = policy[s];\n",
    "    if policy_index == 2\n",
    "        direction = \"right\" \n",
    "    elseif policy_index == 0\n",
    "        direction = \"stop\"\n",
    "    end\n",
    "    \n",
    "    Q_table_data_array[s,2] = direction;\n",
    "    Q_table_data_array[s,3] = policy_index;\n",
    "    \n",
    "    \n",
    "    for a ‚àà 1:length(actions)\n",
    "        Q_table_data_array[s,a+3] = Q_array[s,a];\n",
    "    end\n",
    "end\n",
    "\n",
    "# header -\n",
    "Q_table_header = ([\"State\", \"Direction\", \"œÄ(s)\", \"U(a‚ÇÅ) l\", \"U(a‚ÇÇ) r\"])\n",
    "\n",
    "# show -\n",
    "pretty_table(Q_table_data_array; header = Q_table_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e1a0c-93e0-4a4e-bf54-facfb2fbcd34",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "* [Chapter 7: Mykel J. Kochenderfer, Tim A. Wheeler, Kyle H. Wray \"Algorithms for Decision Making\", MIT Press 2022](https://algorithmsbook.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9eea5-e349-462b-9207-327cd0ac7e0a",
   "metadata": {},
   "source": [
    "### Disclaimer and Risks\n",
    "__This content is offered solely for training and  informational purposes__. No offer or solicitation to buy or sell securities or derivative products, or any investment or trading advice or strategy,  is made, given, or endorsed by the teaching team. \n",
    "\n",
    "__Trading involves risk__. Carefully review your financial situation before investing in securities, futures contracts, options, or commodity interests. Past performance, whether actual or indicated by historical tests of strategies, is no guarantee of future performance or success. Trading is generally inappropriate for someone with limited resources, investment or trading experience, or a low-risk tolerance.  Only risk capital that is not required for living expenses.\n",
    "\n",
    "__You are fully responsible for any investment or trading decisions you make__. Such decisions should be based solely on your evaluation of your financial circumstances, investment or trading objectives, risk tolerance, and liquidity needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
