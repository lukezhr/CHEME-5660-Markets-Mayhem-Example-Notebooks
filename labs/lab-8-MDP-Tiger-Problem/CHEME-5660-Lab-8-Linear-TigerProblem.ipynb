{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35eb2a34-0c2a-435d-b8b9-2bd750239fbc",
   "metadata": {},
   "source": [
    "## CHEME 5660 Lab 8: Formulation and Solution of the Linear Tiger Problem as a Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d3d6c-affb-444c-b52f-39c09c6af15a",
   "metadata": {},
   "source": [
    "<img src=\"./figs/Fig-Linear-MDP-Schematic.png\" style=\"margin:auto; width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d178f-067e-45d5-b16b-f3b21f1b5457",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f1a0a-87f4-47d2-9a57-c0c187ad542f",
   "metadata": {},
   "source": [
    "## Lab 8 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8870fd6-db14-4025-bd34-346573da1dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\"); Pkg.resolve(); Pkg.instantiate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830ceeee-1638-4c48-b8a2-103614e58219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load req'd packages -\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01db2e1-9c7a-4a8c-bb92-20e7f63c769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"CHEME-5660-Lab-8-CodeLib.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e31a09-e973-483b-9609-c858de424830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some global constants -\n",
    "Œ± = 0.50; # probability of moving the direction we are expect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b1df8-e519-4ae9-836b-46113d43c095",
   "metadata": {},
   "source": [
    "#### Configure states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a3e1c5-bcee-4afd-beb1-cf6ca64ec09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the states and actions -\n",
    "safety = 1;\n",
    "tiger = 50;\n",
    "\n",
    "states = range(safety,stop=tiger, step=1) |> collect;\n",
    "actions = [1,2]; # a‚ÇÅ = move left, a‚ÇÇ = move right\n",
    "Œ≥ = 0.95;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6a638-cb44-4a1e-8f0f-2baf3d0d8192",
   "metadata": {},
   "source": [
    "#### Configure the rewards array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4936603-3e2c-464c-8d9a-784121db2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the rewards -\n",
    "R = Array{Float64,2}(undef,length(states), length(actions));\n",
    "\n",
    "# most of the rewards are zero -\n",
    "fill!(R,0.0) # fill R w/zeros\n",
    "\n",
    "# set the rewards for the ends -\n",
    "R[safety + 1,1] = 10; # if in state 2, and we take action 1 = we live, get married, our kids are all doctors, and we are generally content\n",
    "R[tiger-1, 2] = -100; # if in state N - 1, and we take action 2 = we get eaten. Bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e76f5-a5a8-4f80-879b-536fd472dcfe",
   "metadata": {},
   "source": [
    "#### Configure the transition array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de3b2301-0796-4664-8d31-d2d0e67f02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the transitions\n",
    "T = Array{Float64,3}(undef, length(states), length(states), length(actions));\n",
    "fill!(T,0.0);\n",
    "\n",
    "# We need to put values into the transition array (these are probabilities, so eah row much sum to 1)\n",
    "T[safety, 1, 1:length(actions)] .= 1.0; # if we are in state 1, we stay in state 1 ‚àÄa ‚àà ùíú\n",
    "T[tiger, tiger, 1:length(actions)] .= 1.0; # if we are in state 5, we stay in state 5 \n",
    "\n",
    "# left actions -\n",
    "for s ‚àà 2:(tiger - 1)\n",
    "    T[s,s-1,1] = Œ±;\n",
    "    T[s,s+1,1] = (1-Œ±);\n",
    "end\n",
    "\n",
    "# right actions -\n",
    "for s ‚àà 2:(tiger - 1)\n",
    "    T[s,s-1,2] = (1-Œ±);\n",
    "    T[s,s+1,2] = Œ±; \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e26bc1-2a74-451a-85cc-a6d18c0fa7c1",
   "metadata": {},
   "source": [
    "#### Build the MDP problem object and estimate the utility $U^{\\pi}(s)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea12f6c-3bc1-4c9f-9ed8-2c025b156a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_problem = build(MDPProblem; ùíÆ = states, ùíú = actions, T = T, R = R, Œ≥ = Œ≥);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f6c6d4-307c-411a-b55c-10c032963259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a always right policy -\n",
    "always_move_right(s) = 2;\n",
    "always_move_left(s) = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e2a978a-aa71-4ef9-a4b2-221804db6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = iterative_policy_evaluation(mdp_problem, always_move_right, 20*length(states));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169766e-885e-424b-a7e4-ad7dc292c828",
   "metadata": {},
   "source": [
    "#### Estimate the Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c20ce52-ade6-4766-aa10-3f00aabff501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48√ó2 Matrix{Float64}:\n",
       "    9.99998        -1.84825e-5\n",
       "   -3.89105e-5     -3.89105e-5\n",
       "   -6.34344e-5     -6.34344e-5\n",
       "   -9.46356e-5     -9.46356e-5\n",
       "   -0.000135798    -0.000135798\n",
       "   -0.000191256    -0.000191256\n",
       "   -0.000266845    -0.000266845\n",
       "   -0.000370524    -0.000370524\n",
       "   -0.000513205    -0.000513205\n",
       "   -0.000709908    -0.000709908\n",
       "   -0.000981338    -0.000981338\n",
       "   -0.00135607     -0.00135607\n",
       "   -0.00187354     -0.00187354\n",
       "    ‚ãÆ            \n",
       "   -4.36306        -4.36306\n",
       "   -6.02676        -6.02676\n",
       "   -8.32486        -8.32486\n",
       "  -11.4993        -11.4993\n",
       "  -15.8841        -15.8841\n",
       "  -21.941         -21.941\n",
       "  -30.3074        -30.3074\n",
       "  -41.8641        -41.8641\n",
       "  -57.8275        -57.8275\n",
       "  -79.8781        -79.8781\n",
       " -110.337        -110.337\n",
       "  -52.41         -152.41"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_array = Q(mdp_problem, U)[2:end-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b3c984-15a2-4c3b-abba-11364be11f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ‚ãÆ\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = œÄ(Q_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9eea5-e349-462b-9207-327cd0ac7e0a",
   "metadata": {},
   "source": [
    "### Disclaimer and Risks\n",
    "__This content is offered solely for training and  informational purposes__. No offer or solicitation to buy or sell securities or derivative products, or any investment or trading advice or strategy,  is made, given, or endorsed by the teaching team. \n",
    "\n",
    "__Trading involves risk__. Carefully review your financial situation before investing in securities, futures contracts, options, or commodity interests. Past performance, whether actual or indicated by historical tests of strategies, is no guarantee of future performance or success. Trading is generally inappropriate for someone with limited resources, investment or trading experience, or a low-risk tolerance.  Only risk capital that is not required for living expenses.\n",
    "\n",
    "__You are fully responsible for any investment or trading decisions you make__. Such decisions should be based solely on your evaluation of your financial circumstances, investment or trading objectives, risk tolerance, and liquidity needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
